<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ML advice</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h2 id="advice-for-machine-learning-phd-students">Advice for machine learning PhD students</h2>
<p>by <a href="https://markusheinonen.github.io/">Markus Heinonen</a>, Aalto University, 2025</p>
<p>Here’s what I learned doing a machine learning PhD and supervising many others.</p>
<p>Accompanying <a href="https://markusheinonen.github.io/research-tips.pptx">slideset</a></p>
<p><strong>Listen to big-shots</strong></p>
<ul>
<li>
<p>Andrej Karpathy <a href="https://karpathy.github.io/2016/09/07/phd/">A Survival Guide to a PhD</a></p>
</li>
<li>
<p>Patrick Kidger <a href="https://kidger.site/thoughts/just-know-stuff/">Just know stuff</a></p>
</li>
<li>
<p>John Schulman <a href="http://joschu.net/blog/opinionated-guide-ml-research.html">Opinionated guide to ML research</a></p>
</li>
<li>
<p>Bill Freeman <a href="https://people.csail.mit.edu/billf/publications/How_To_Do_Research.pdf">How to do research</a></p>
</li>
<li>
<p>Eamonn Keogh <a href="https://www.cs.ucr.edu/~eamonn/Keogh_SIGKDD09_tutorial.pdf">How to do good research</a></p>
</li>
<li>
<p>Stefano Albrecht <a href="https://agents-lab.org/phd-handbook/">PhD Handbook</a></p>
</li>
</ul>
<p><strong>The three pillars.</strong> Machine learning is (1) understanding, (2) communicating, and (3) implementing; in this order of importance. To minimize time spent coding one should maximize time spent on understanding the problems and the solutions.</p>
<p><strong>Clarity is all you need</strong>. Research often boils down to simplifying the problems, solutions and ideas to their simplest version. It’s a good idea to refactor and rework the math until it’s so clear that an outsider could grasp the ideas at ease from a first glance. This often takes time, but will help make a best realisation of the contribution. Implementing, writing and publishing becomes much easier after the conceptual work is done.</p>
<p><strong>Become a coder wiz</strong>. Learn to automate your workflows: code, experiments, runs, logging, analysis, plotting, results. Make sure you can reconfig and restart and reanalyse your experiments in "one click" against a GPU cluster. Make clean code and refactor often. Learn the latest tools and frameworks. Ask your collegues for their best practises. <strong>One can’t survive a PhD anymore without being a proficient ML engineer</strong>. This will multiply your research output.</p>
<p><strong>Exploit LLMs</strong>. Use LLMs to code faster, write better papers, do literature surveys, and solve math. LLMs are a game-changer in how we do science.</p>
<p><strong>Benchmarks are not science</strong>. Paper’s should not be treated as benchmark competitions, but opportunities to identify novel research problems and understand and address their root causes. Benchmark tables are not scientifically interesting: every year new methods come up and errors go down (brrrr..), often with little gain in insights. Instead aim at understanding the qualitative improvement behind the contributions, or finding gaps in literature, or problems behind SOTA models. These often come from understanding related works and your model more in-depth. By solving true open problems the SOTA results will follow.</p>
<p><strong>Focus on problems</strong>. Instead of finding solutions, focus on finding problems that are true, novel and significant. Find open problems by looking at what state-of-the-art can’t do, does poorly, or ignores.</p>
<p><strong>Make a point</strong>. Don’t just present a method with 2% better performance. So what? There are thousands of papers every year: why should one care about a 2% improvement? Why is this important and significant; what do we learn from this; why should everyone know this method?</p>
<p><strong>Don't be pretentious</strong>. Math for math's sake is rarely useful in machine learning. Don't drown your audience in theory unless it matters. Keep things as simple and tied to the real world problem you are solving.</p>
<p><strong>It’s a marathon</strong>. PhD is around 1000 days of work. Plan long term and check your progress twice a year.</p>
<p><strong>Keep learning</strong>. One needs to become world’s top expert in the phd topic during it. This means reading 100’s of papers during your phd. Most scientists know one thing very well, and apply it everywhere. For instance, differential geometry, Bayes, numerics, etc. This makes publishing much easier.</p>
<p><strong>Do your homework</strong>. Follow good ML principles and keep the quality bar high. Don’t make shortcuts. This will come around and cause trouble in future. Understand your own code and data and literature throughout. Don't rush to implementing a cool idea before you've gone through all related papers. Prepare for any question a collegue could have. Don’t submit unfinished manuscripts with sloppy presentation and incomplete results.</p>
<p><strong>Debug to understand</strong>. When things are not working, visualise everything: the loss, the optimisation, the network, the activations, the weights, the data, the likelihood, the gradients, the layers, etc. A “that’s odd…” moment will come.</p>
<p><strong>Backtrack to debug</strong>. When things don't work, backtrack until you find a solid foundations that you fully understand and that fully works as expected in every possible way. Then, start adding your stuff back in one at a time: verifying and checking each.</p>
<p><strong>Do project reviews</strong>. Present your ideas, projects and code to other phd students for honest feedback. You will learn a ton. If this doesn’t exist in your lab, make it exist. Do this often, eg. once a month.</p>
<p><strong>How to present</strong>. Make slides for every meeting. Include a context slide. Distill your ideas to their simplest version: what are the main points you need to convey? Great slides usually have 1 picture and little text (closer 10 words than 100 per slide). Distribute meeting agenda beforehand and summary afterward.</p>
<p><strong>How to give a talk</strong>. Math-heavy talks are pointless: the experts in audience already know your work, and rest can’t follow. Talks should be aimed at a non-expert audience to inspire them about stuff they don’t know about. Spend third of your time giving a big picture of the domain: why is it important and cool? Spend rest on the specific problem, and your high-level ideas. Remember that an average listener has mental budget for max 5 equations, and will stop listening if you present more.</p>
<p><strong>Slow down to speed up</strong>. Spend time understanding the problem you want to solve, and verify it exists. Formulate hypotheses on how to improve. Start from trivial baselines (random forest, linear regression), simple neural networks, and pre-trained SOTA baselines. Run a sequence of more and more complex models, where ideally you change and quantify only one thing at a time. You want to do coordinate descent: move along one design axis at a time. See Andrej Karpathy’s neural network <a href="http://karpathy.github.io/2019/04/25/recipe/">training recipe</a> and Google’s <a href="https://github.com/google-research/tuning_playbook">DL tuning playbook</a>.</p>
<p><strong>Don’t tell me “it doesn’t work”</strong>. Why doesn’t it? What steps did you make to narrow down where the problem lies?</p>
<p><strong>Break your model</strong>. Stress-test your model until it breaks. What are its limits? This gives you direct avenue to making a second paper. Look at the <a href="https://arxiv.org/abs/2001.02478">XAI question bank</a>.</p>
<p><strong>Make first-author papers</strong>. To have a PhD and a career afterwards, you need to focus on making first-author papers where you were the driving force. A paper record of middle-author papers indicates poor priorities and inability to deliver.</p>
<p><strong>Don’t hide from your supervisors</strong>. Supervisors love talking about science, being challenged, and hearing about your ideas. Actively ask for advice and feedback from your supervisor: meetings where only you talk benefit no one. Insist on regular update meetings.</p>
<p><strong>Be honest</strong>. Tell your supervisor when you don’t understand something or when you are struggling. Don’t nod if you didn’t understand; ask for clarification. Implying otherwise makes it difficult to work with you. Don’t imply that you are doing fine when you aren’t. Never cancel meetings.</p>
<p><strong>Go to conferences</strong>. Attend a top conference in your field (NeurIPS/ICML/ICLR/etc) every year, even if you have no paper. Workshops are a great way to get your foot in, and practise presenting. Prepare a 5 second and 60 second pitch of what you do to introduce yourself.</p>
<p><strong>Be visible</strong>. Have a website for collegues and bigshots to find you. If you have no papers yet, having a technical blog is a good way to show your expertise (<a href="https://lilianweng.github.io/">great example</a>).</p>
<p><strong>Advertise your papers</strong>. Make a website for each paper you make (a good <a href="https://www.guandaoyang.com/PointFlow/">example</a>). Release the code, and spend time polishing it, making demos, tutorials and notebooks. Write a friendly <a href="https://yang-song.net/blog/2021/score/">blog post</a> for each of your papers. The more user-friendly your method is, the more citations and impact you will get. Often the most famous method in a field is not the best, but the one that is easiest to use and has best documentation.</p>
<p><strong>Write simple papers.</strong> Write in a way that is accessible to a non-expert reader. Use illustrations, colors and short paragraphs. Use LLMs to polish the language. Shorten, distill and simplify as much as you can. If you can’t write a simple paper, the idea is not yet ready. Be explicit and use precise math. Organise internal mock peer-review with other students, especially ones not from your field since reviewers are chosen randomly.</p>
<p><strong>Spend time on figures.</strong> Make a good "abstract" figure for page 1 that illustrates the main innovation. Animations typically make even the most complex ideas understandable; work on them (see <a href="https://www.guandaoyang.com/PointFlow/">here</a> or <a href="https://yang-song.net/blog/2021/score/">here</a>).</p>
<p><strong>Go to the point</strong>. Write what you want to say, and nothing more. As a reader I want to see a 5 line abstract and 2 paragraph introduction: please do this. Bullet your contributions. Related works often works best at the end of the paper. Use short paragraphs, and use \paragraph to title them. Papers often have a good flow when whitespace is maximised. Make a feature table wrt related methods (<a href="https://arxiv.org/abs/1810.01367">example</a>). Add conclusions in boxes (<a href="https://arxiv.org/abs/2104.14421">example</a>). Color equations (<a href="https://arxiv.org/abs/2303.00848">example</a>). Use \underbrace copiously. Put math in full lines. Make figures and tables self-contained. Annotate everything about figures. Captions should give the conclusion, not the description of the figure. Figure font size should not decrease from paper: use small 'figsize'. Include standard deviations. Put full math derivations in appendix, no matter how simple or basic.</p>
<p><strong>Write transparent papers.</strong> You want to make everything about the work transparent. Visualise the data by showing arrays of example datapoints, showing summary statistics (shapes, sizes, means, variances, etc), and global visualisations (pca, tsne, umap). Visualise the training by showing optimisation traces of all competing methods and of all seeds and repeats. Visualise the predictions: show examples, logit distributions, statistics (bias, variance), etc. Make sure the reader can mentally trace the entire method pipeline from data to results. Use figures to illustrate and concretise the ideas (see <a href="https://arxiv.org/abs/1912.04958">example</a>).</p>
<p><strong>Follow the domain</strong>. Check all orals, keynotes and tutorials of all top ML conferences, even if they don’t relate to your research. This tells where the field is moving, and you always get some useful ideas. Follow what your competitors are publishing. Use Google Scholar to follow seminal papers and their forward citations.</p>
<p><strong>Attend a summer school</strong>. Go to one on your first year.</p>
<p><strong>Do an internship</strong>. Company internships or research lab visits are very useful. Most labs are happy to receive students, while company processes are stochastic. Apply early and often. Typical visits are three months.</p>
<p><strong>Enjoy what you do</strong>. Move towards projects that interest you. Finish your current project regardless. If your project is not progressing, take initiative. This is your phd thesis and career, you need to drive it forward. Don’t expect a supervisor to make the phd happen: their career does not depend on your success.</p>
<p><strong>If you are stuck</strong>. Slow down, rethink what you are doing, and discuss with your collegues (you will notice that people love to give advice!): what problem are you solving and is it the right problem? If the problem is right, is the solution? Keep reading literature: all ML problems have already been solved by someone in some paper in some domain.</p>
<p><strong>Organize your time</strong>. Make sure to spend at least 20% of your time reading. Do not slip from this. Keep a research diary and a technical report on your project. Share these as a persistent single-click url for your supervisors.</p>
<p><strong>Queue your work</strong>. Research is a sequence of small tasks. Treat it as FIFO queue: have a single active task at a time, and conclude and close it before you move forward. Do not multitask. Do not leave unfinished tasks. If your backlog is growing, stop, and resolve them first.</p>
<p><strong>Calendar, not TODO lists</strong>. Don’t make TODO lists. They expand until they become unbearable, and you restart. Instead, block time for tasks on your calendar. Follow <a href="https://deviparikh.substack.com/p/calendar-in-stead-of-to-do-lists-9ada86a512dd">Devi Parikh’s advice</a>.</p>
<p><strong>Study math</strong>. You want to understand algebra, calculus, probability, statistics, measure theory, functional analysis, differential geometry, complex analysis, and optimisation.</p>
<p><strong>The ideal story</strong>. Science builds incrementally on top of earlier results. The earlier state-of-the-art is the bedrock, and your contribution sits on top. A supervisor probably expects to see these steps:</p>
<ol>
<li>
<p>Understand the research domain, and read all papers on it</p>
</li>
<li>
<p>Be able to reproduce earlier, state-of-the-art results</p>
</li>
<li>
<p>Demonstrate a significant short-coming in state-of-the-art</p>
</li>
<li>
<p>Find or adapt a known solution to the type of problem</p>
</li>
</ol>
<p><strong>Read books</strong>. Reading textbooks cover-to-cover is useful to obtain holistic knowledge. You want to understand mathematical foundations (Deisenroth), statistical learning (Hastie), machine learning (Bishop) and deep learning (Murphy). If you only read one book, go for Bishop. Great books are:</p>
<ul>
<li>
<p>On math: Deisenroth et al <a href="https://mml-book.github.io/">Mathematics for machine learning</a></p>
</li>
<li>
<p>On modern ML: Murphy <a href="https://probml.github.io/pml-book/">Probabilistic machine learning</a> series, Bishops’ <a href="https://www.bishopbook.com/">Deep Learning</a></p>
</li>
<li>
<p>On learning theory: Mohri et al <a href="https://cs.nyu.edu/~mohri/mlbook/">Foundations of machine learning</a>, Shalev-Shwartz et al <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/">Understanding Machine Learning</a></p>
</li>
<li>
<p>On statistical learning: Hastie et al <a href="https://hastie.su.domains/ElemStatLearn/">Elements of Statistical Learning</a></p>
</li>
<li>
<p>On Bayesian modelling: Gelman et al <a href="https://sites.stat.columbia.edu/gelman/book/">Bayesian Data Analysis</a></p>
</li>
<li>
<p>On generative models: Tomczak <a href="https://jmtomczak.github.io/dgm_book.html">Deep Generative Models</a></p>
</li>
<li>
<p>On information theory: MacKay <a href="https://www.inference.org.uk/mackay/itila/book.html">Information theory, Inference and Learning Algorithms</a></p>
</li>
</ul>
</div>
</body>

</html>
